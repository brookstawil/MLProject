{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 - Country Flags\n",
    "The dataset below provides different information of a country in order to predict potentially what the majority religion is. This falls more in line with a clustering problem, whereby flags are clustered into groups.\n",
    "https://archive.ics.uci.edu/ml/datasets/Flags?fbclid=IwAR3cI_9sS9XxKBJ-RPXEIAPBOS3QDqkS7qYxicM6F_TiJB--5P8r1Tt6Lxk\n",
    "\n",
    "#### Problem\n",
    "For this problem we will find clusters of flags based on the data attributes, find the common characteristics, and output the connections that we find between the flags, in this case, the majority religion of the country\n",
    "\n",
    "#### Extending the problem\n",
    "It would be interesting to see what our algorithm would cluster a new-made up flag that we create based on our own human biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "This dataset is stored on a single csv file across all of the features. Numpy can easily load in these values int oa matrix so that we can use it in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 194)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filedata = np.genfromtxt('./data/CountryFlags/flag.data', dtype=None, delimiter=',', encoding='utf-8')\n",
    "\n",
    "data = [[None for _ in range(len(filedata[0]))] for _ in range(len(filedata))]\n",
    "\n",
    "# Data is stored as mostly integers, but these correspond to a string in the data description, stored in these lists \n",
    "landmass = [None, 'N.America', 'S.America', 'Europe', 'Africa', 'Asia', 'Oceania']\n",
    "quadrant = [None, 'NE', 'SE', 'SW', 'NW']\n",
    "languages = [None, 'English', 'Spanish', 'French', 'German', 'Slavic', 'Other Indo-European', 'Chinese', 'Arabic', 'Japanese/Turkish/Finnish/Magyar', 'Others']\n",
    "religions = ['Catholic', 'Other Christian', 'Muslim', 'Buddhist', 'Hindu', 'Ethnic', 'Marxist', 'Others']\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        # Country Name\n",
    "        if (j == 0):\n",
    "            data[i][j] = str(filedata[i][j])\n",
    "        # Landmass\n",
    "        elif (j == 1):\n",
    "            data[i][j] = landmass[filedata[i][j]]\n",
    "        elif (j  == 2):\n",
    "            data[i][j] = quadrant[filedata[i][j]]\n",
    "        elif (j  == 5):\n",
    "            data[i][j] = languages[filedata[i][j]]\n",
    "        elif (j == 6):\n",
    "            data[i][j] = religions[filedata[i][j]]\n",
    "        else:\n",
    "            data[i][j] = filedata[i][j]\n",
    "        # Make the row into a numpy array\n",
    "        data[i] = np.array(data[i])\n",
    "\n",
    "# Transpose so that features are along the rows and data points are along the columns\n",
    "data = np.array(data).transpose()\n",
    "\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Dataset\n",
    "The point of this problem is to only use the data and features that we can get from a given countries flag. This dataset includes features such as population, density etc. that are not related to the flag, and should be removed.\n",
    "One of these, religion, will be our label that we are aiming to predict based off of the flag. Thus we will have to extract the religion feature as a label, and eliminate the non-flag related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Afghanistan' 'Albania' 'Algeria' ... 'Zaire' 'Zambia' 'Zimbabwe']\n",
      " ['Asia' 'Europe' 'Africa' ... 'Africa' 'Africa' 'Africa']\n",
      " ['NE' 'NE' 'NE' ... 'SE' 'SE' 'SE']\n",
      " [648 29 2388 ... 905 753 391]\n",
      " [16 3 20 ... 28 6 8]\n",
      " ['Others' 'Other Indo-European' 'Arabic' ... 'Others' 'Others' 'Others']]\n"
     ]
    }
   ],
   "source": [
    "# Extract the religions as the labels, row 6\n",
    "names = data[0]\n",
    "labels = data[6] \n",
    "data = np.delete(data, 6, axis=0)\n",
    "\n",
    "# Extract the non flag related data\n",
    "print (data[0:6])\n",
    "for i in range(6):\n",
    "    data = np.delete(data, 0, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing our Cleaned Data\n",
    "\n",
    "With our data cleaned and prepared for analysis, we can start our analysis. Our goal is to build a classifier that based on these features about a country's flag, we can classify that country's major religion. It may be interesting to see what would happen if we used another feature as a target, but for this project we will be solely be focused on a classifier that focuses on religion.\n",
    "These are the 3 methods that will be used, along with the group member responsible for that method:\n",
    "- K-Nearest Neighbors - Brooks Tawil\n",
    "- Naive Bayes - Jack Chiu\n",
    "- Kernal SVM - Gavin Mckim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors - Brooks Tawil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 23)\n",
      "[[0 3 5 ... 0 0 0]\n",
      " [0 0 3 ... 0 1 1]\n",
      " [2 0 3 ... 0 2 2]\n",
      " ...\n",
      " [0 0 4 ... 0 2 0]\n",
      " [3 0 4 ... 0 2 7]\n",
      " [0 7 5 ... 0 2 0]]\n",
      "['Muslim' 'Marxist' 'Other Christian' 'Catholic' 'Ethnic' 'Buddhist'\n",
      " 'Hindu' 'Others']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def uNiQuE(vec):\n",
    "    popCtr = 0\n",
    "    for p in vec:\n",
    "        if p in vec[:popCtr]:\n",
    "            vec = np.delete(vec, popCtr, 0)\n",
    "            popCtr -= 1\n",
    "        popCtr += 1\n",
    "        \n",
    "    return vec \n",
    "\n",
    "def preProcess(data):\n",
    "    i = 0\n",
    "    for q in range(len(data)):\n",
    "        if None in data[i]:\n",
    "            data = np.delete(data, i, 0)\n",
    "            i -= 1        \n",
    "        i += 1\n",
    "        for j in range(len(data[i-1])):\n",
    "            if type(data[i-1,j]) == str:\n",
    "                data[i-1,j] = data[i-1,j].lower()\n",
    "\n",
    "    domColor = uNiQuE(data[:,10])\n",
    "    topLeftColor = uNiQuE(data[:,-2])\n",
    "    botRightColor = uNiQuE(data[:,-1])\n",
    "    numStars = np.array([6, 5, 4, 3, 2, 1, 0])\n",
    "    for i in range(len(data)):\n",
    "        tempInd = np.where(domColor == data[i,10])\n",
    "        data[i,10] = int(tempInd[0])\n",
    "        tempInd = np.where(topLeftColor == data[i,-2])\n",
    "        data[i,-2] = int(tempInd[0])\n",
    "        tempInd = np.where(botRightColor == data[i,-1])\n",
    "        data[i,-1] = int(tempInd[0])\n",
    "        \n",
    "        #make stars be in range [0 to >5]\n",
    "        tempInd = np.where(numStars <= data[i,15])\n",
    "        data[i,15] = int(tempInd[0][0])\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            data[i,j] = int(data[i,j])\n",
    "    return data\n",
    "\n",
    "cleanData = preProcess(data.T)\n",
    "religions = uNiQuE(labels)\n",
    "\n",
    "print (cleanData.shape)\n",
    "print (cleanData)\n",
    "print (religions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Euclidean Distance as the distance metric. other distance metrics, such as Manhattan Distance, do exist and could also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(trainingInstance, testInstance, length):\n",
    "    distance = 0.0    \n",
    "    for x in range(length):\n",
    "        distance += np.square(trainingInstance[x] - testInstance[x])\n",
    "    \n",
    "    return math.sqrt(distance)\n",
    "\n",
    "# Defining our KNN model\n",
    "def knn(trainingSet, testInstance, k):\n",
    "    # The training set will have the labels appended to the end, so that sorting can be done with the labels attached\n",
    "    distances = {}\n",
    "    sort = {}\n",
    " \n",
    "    length = len(testInstance)\n",
    "    \n",
    "    # Calculating euclidean distance between each row of training data and test data\n",
    "    for x in range(len(trainingSet)):\n",
    "        distances[x] = euclideanDistance(trainingSet[x], testInstance.T, length)\n",
    "          \n",
    "    # Sorting them on the basis of distance\n",
    "    sorted_d = sorted(distances.items(), key=operator.itemgetter(1))\n",
    " \n",
    "    # Extracting top k neighbors\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(sorted_d[x])\n",
    "    classVotes = {}\n",
    "    \n",
    "    # Calculating the most freq class in the neighbors\n",
    "    for x in range(len(neighbors)):\n",
    "        response = trainingSet[neighbors[x][0]][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # Retrun as tuple (Class, List Of Neighbors)\n",
    "    return(sortedVotes[0][0], neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross Validation\n",
    "\n",
    "For this dataset, we don't have that many country flags to use. We only have 194 country flags to work with! In addition, the use of kNN means that we especially want to have a large training set, so that our distances between our training and test instances are not that far from the expected reality, with holes in our training. In addition, our dataset is relatively small, so with a modern system the computation will not take long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'operator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d0329053d55e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Run knn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0massignment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataWithLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestInstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Check if the assignemnt is correct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-3ac9bac14394>\u001b[0m in \u001b[0;36mknn\u001b[1;34m(trainingSet, testInstance, k)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Sorting them on the basis of distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0msorted_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Extracting top k neighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'operator' is not defined"
     ]
    }
   ],
   "source": [
    "# Run KNN with Leave-One-Out Cross Validation\n",
    "kRange = range(1, 12)\n",
    "missClasses = [0 for _ in range(len(kRange))]\n",
    "correctClasses = [0 for _ in range(len(kRange))]\n",
    "accuracies = [0 for _ in range(len(kRange))]\n",
    "\n",
    "# Leave-One-Out CV, changing the k in kNN\n",
    "for k in kRange:\n",
    "    for i in range(len(cleanData)):\n",
    "         # Re-make our dataWithLabels\n",
    "        dataWithLabels = list(cleanData.T)\n",
    "        np.array(dataWithLabels.append(labels.T))\n",
    "        dataWithLabels = np.array(dataWithLabels).T\n",
    "        \n",
    "        # Assign a single test instance\n",
    "        testInstance = cleanData[i-1]\n",
    "        testInstanceLabel = labels[i-1]\n",
    "\n",
    "        # We will only delete from dataWithLabels\n",
    "        dataWithLabels = np.delete(dataWithLabels, i-1, axis=0)\n",
    "        \n",
    "        # Run knn\n",
    "        assignment, neighbors = knn(dataWithLabels, testInstance, k)\n",
    "        \n",
    "        # Check if the assignemnt is correct\n",
    "        if (assignment == testInstanceLabel):\n",
    "            correctClasses[k-1] += 1\n",
    "        else:\n",
    "            missClasses[k-1] += 1\n",
    "\n",
    "print (missClasses)\n",
    "print (correctClasses)\n",
    "\n",
    "# Compute the accuracies for each fold and report\n",
    "for i in range(len(accuracies)):\n",
    "    accuracies[i] = correctClasses[i]/float(correctClasses[i] + missClasses[i])\n",
    "    \n",
    "print (accuracies)\n",
    "best_k = listkRange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - Jack Chui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uNiQuE(vec):\n",
    "    popCtr = 0\n",
    "    for p in vec:\n",
    "        if p in vec[:popCtr]:\n",
    "            vec = np.delete(vec, popCtr, 0)\n",
    "            popCtr -= 1\n",
    "        popCtr += 1\n",
    "        \n",
    "    return vec   \n",
    "\n",
    "def kFold(data, labels, kFolds):\n",
    "    #shuffle\n",
    "    inds = np.random.choice(np.arange(len(data)), len(data))\n",
    "    data[:] = data[inds]\n",
    "    labels[:] = labels[inds]\n",
    "\n",
    "    startInd = 0\n",
    "    stepSize = int(len(data)/kFolds)\n",
    "    Acc = []\n",
    "    predictions = []\n",
    "    for i in range(kFolds):\n",
    "        if i != kFolds-1:\n",
    "            testData = data[startInd:startInd+stepSize]\n",
    "            testLabels = labels[startInd:startInd+stepSize]\n",
    "            trainingData = data[:startInd]\n",
    "            trainingData = np.concatenate((trainingData, data[startInd+stepSize:]))\n",
    "            trainingLabels = labels[:startInd]\n",
    "            trainingLabels = np.concatenate((trainingLabels,labels[startInd+stepSize:]))\n",
    "        else:\n",
    "            testData = data[startInd:]\n",
    "            testLabels = labels[startInd:]\n",
    "            trainingData = data[:startInd]\n",
    "            trainingLabels = labels[:startInd]\n",
    "\n",
    "        startInd += stepSize\n",
    "        temp, pList = calcErrNaive(trainingData, trainingLabels, testData, testLabels)\n",
    "        Acc.append(temp)\n",
    "        predictions.extend(pList)\n",
    "    \n",
    "    return Acc, labels, predictions\n",
    "        \n",
    "\n",
    "def trainNaive(data, labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    prior = counts\n",
    "    prior = (prior+0.0)/len(data)\n",
    "    \n",
    "    conditional = np.zeros((8, len(data[0]), 50))\n",
    "    #conditional = (labels, feature, values in feature)\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            conditional[labels[i], j, data[i,j]] += 1\n",
    "    for i in range(len(conditional)):\n",
    "        for j in range(len(conditional[0])):\n",
    "            sumCondition = 0\n",
    "            for k in range(0,conditional.shape[0]):\n",
    "                sumCondition += sum(conditional[k,j,:])\n",
    "            for k in range(0,len(conditional[0,j])):\n",
    "                conditional[i,j,k] = conditional[i,j,k]/sumCondition\n",
    "    \n",
    "\n",
    "    return prior, conditional, unique\n",
    "\n",
    "\n",
    "def testNaive(prior, conditional, unique, sample):\n",
    "    prob = prior\n",
    "    for i in range(len(sample)):\n",
    "        for j in range(len(prob)):\n",
    "            prob[j] = prob[j] * conditional[j,i,sample[i]]\n",
    "    maxVal = np.argmax(prob)\n",
    "        \n",
    "    return unique[maxVal]\n",
    "        \n",
    "def calcErrNaive(trainingData, trainingLabels, testData, testLabels):\n",
    "    errs = 0\n",
    "    prior, conditional, unique = trainNaive(trainingData, trainingLabels)\n",
    "    pList = []\n",
    "    for i in range(len(testData)):\n",
    "        prediction = testNaive(prior, conditional, unique, testData[i])\n",
    "        errs += int(prediction == testLabels[i])\n",
    "        pList.append(prediction)\n",
    "        \n",
    "    return np.round(errs/len(testLabels), 6), pList\n",
    "\n",
    "\n",
    "def preProcess(data):\n",
    "    i = 0\n",
    "    for q in range(len(data)):\n",
    "        if None in data[i]:\n",
    "            data = np.delete(data, i, 0)\n",
    "            i -= 1        \n",
    "        i += 1\n",
    "        for j in range(len(data[i-1])):\n",
    "            if type(data[i-1,j]) == str:\n",
    "                data[i-1,j] = data[i-1,j].lower()\n",
    "\n",
    "    domColor = uNiQuE(data[:,10])\n",
    "    topLeftColor = uNiQuE(data[:,-2])\n",
    "    botRightColor = uNiQuE(data[:,-1])\n",
    "    numStars = np.array([6, 5, 4, 3, 2, 1, 0])\n",
    "    for i in range(len(data)):\n",
    "        tempInd = np.where(domColor == data[i,10])\n",
    "        data[i,10] = int(tempInd[0])\n",
    "        tempInd = np.where(topLeftColor == data[i,-2])\n",
    "        data[i,-2] = int(tempInd[0])\n",
    "        tempInd = np.where(botRightColor == data[i,-1])\n",
    "        data[i,-1] = int(tempInd[0])\n",
    "        \n",
    "        #make stars be in range [0 to >5]\n",
    "        tempInd = np.where(numStars <= data[i,15])\n",
    "        data[i,15] = int(tempInd[0][0])\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            data[i,j] = int(data[i,j])\n",
    "    return data\n",
    "\n",
    "tempData = preProcess(data.T)\n",
    "religions = uNiQuE(labels)\n",
    "tempLabels = labels[:]\n",
    "for i in range(len(labels)):\n",
    "    tempInd = np.where(religions == labels[i])\n",
    "    tempLabels[i] = int(tempInd[0])\n",
    "    \n",
    "for i in np.arange(3,13,2):\n",
    "    acc, actual, predictions = kFold(tempData, tempLabels, i)\n",
    "    print('k = ', i, ' with average accuracy = ' , np.average(acc).round(6))\n",
    "    print('Accuracy for each fold: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes was created to classify from the data what type of religion the country with that flag would have. Evidently, from running the classifier with k-fold cross validation over various k values, the accuracy of the classifier was pretty low $\\leq 40\\%$. It can be said that there is a pretty weak relation between the country's flag features/data and the majority religion of that country based on the Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, actual, predictions = kFold(tempData, tempLabels, 11)\n",
    "print('Actual Religions and Predicted Religion for first 20 samples: ')\n",
    "for i in range(20):\n",
    "    if actual[i] == 2:\n",
    "        print('Actual: ', religions[actual[i]], '\\t Predicted:', religions[predictions[i]])\n",
    "    elif actual[i] == 6:\n",
    "        print('Actual: ', religions[actual[i]], '\\t \\t \\t Predicted:', religions[predictions[i]])\n",
    "    else:\n",
    "        print('Actual: ', religions[actual[i]], '\\t \\t Predicted:', religions[predictions[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Naive Bayes Classifier, the classifier tends to classify the flags as majority Muslim religion. Thus, based on Naive Bayes, there seems to not be much relation between a flag's image and its country's religion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernal SVM - Gavin McKim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "sdata = data[0:21,:]\n",
    "sdata = np.delete(sdata, 10, 0)\n",
    "#Following the preprocessing done by Brooks, I did a little of my own preprocessing. I just got rid of all the features\n",
    "# that don't have numerical values.\n",
    "\n",
    "#print(np.shape(sdata))\n",
    "#print(sdata)\n",
    "shfl = np.vstack((sdata,np.reshape(labels,[1,194])))\n",
    "shfl = np.random.permutation(shfl.T).T\n",
    "sdata = shfl[0:20,:]\n",
    "labels = shfl[20,:]\n",
    "\n",
    "#print(shfl)\n",
    "\n",
    "\n",
    "#print(sdata[8,:])\n",
    "G = np.arange(.01,1,0.01)\n",
    "\n",
    "#for c in C:\n",
    "means = []\n",
    "for g in G:\n",
    "    #The Kernel used is Radia Bias Kernel. I have a loop to find the optimal value of the tuning parameter G(or gamma in\n",
    "    # the SVC call. I have decided to not tune the penalty parameter C because it makes the runtime too long on my computer).\n",
    "    clf = svm.SVC(C=1, kernel='rbf', gamma=g)\n",
    "\n",
    "    acc = []\n",
    "\n",
    "\n",
    "    # 5-Fold Cross Validation\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            testing = sdata[:,0:39]\n",
    "            training = sdata[:,39:194]\n",
    "            yf = labels[39:194]\n",
    "            yt = labels[0:39]\n",
    "        if i == 1:\n",
    "            testing = sdata[:,39:78]\n",
    "            training = np.concatenate((sdata[:,0:39],sdata[:,78:194]), axis=1)\n",
    "            yf = np.concatenate((labels[0:39],labels[78:194]), axis=0)\n",
    "            yt = labels[39:78]\n",
    "        if i == 2:\n",
    "            testing = sdata[:,78:117]\n",
    "            training = np.concatenate((sdata[:,0:78],sdata[:,117:194]), axis=1)\n",
    "            yf = np.concatenate((labels[0:78],labels[117:194]), axis=0)\n",
    "            yt = labels[78:117]\n",
    "        if i == 3:\n",
    "            testing = sdata[:,117:156]\n",
    "            training = np.concatenate((sdata[:,0:117],sdata[:,156:194]), axis=1)\n",
    "            yf = np.concatenate((labels[0:117],labels[156:194]), axis=0)\n",
    "            yt = labels[117:156]\n",
    "        if i == 4:\n",
    "            testing = sdata[:,156:194]\n",
    "            training = sdata[:,0:156]\n",
    "            yf = labels[0:156]\n",
    "            yt = labels[156:194]\n",
    "\n",
    "        clf.fit(training.T, yf)\n",
    "        acc.append(clf.score(testing.T,yt))\n",
    "    means.append(np.mean(acc))\n",
    "\n",
    "opt = np.argmax(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy is: 0.3137651821862348\n",
      "This accuracy is achieved by having a gamma term of: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"The highest accuracy is:\",means[opt])\n",
    "print(\"This accuracy is achieved by having a gamma term of:\", (opt/100)+.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
