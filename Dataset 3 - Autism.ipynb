{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3 - Autism\n",
    "\n",
    "This dataset provides 20 attributes to help determine if an adult could be on the autistic spectrum or have ASD. The data provided is based upon autism screening of adults in contrast to most other datasets based on behavior traits. In the dataset, 10 behavioral and 10 individual characteristics are provided.\n",
    "https://archive.ics.uci.edu/ml/datasets/Autism+Screening+Adult\n",
    "\n",
    "#### Problem\n",
    "Our goal is to create a classifier, that can diagnose autism based on the answers to certain questions and physical characteristics. This is a classification problem and very much mirrors our hypothetical situations of diagnosing cancer that we discuss in class. Except our features are more based around psychological evaluation and not physical traits and attributes of a physical ailment.\n",
    "\n",
    "#### Extending the problem\n",
    "Out of all of the datasets, this one has the most social impact, and the creation of such an algorithm is most likely already the subject of study in various academic realms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "The dataset is laid out in a .arff file and needs to be loaded properly so that we can use it. Fortunately, Scipy has a function to hanle this.\n",
    "\n",
    "Some features, like the answers to test questions, are binary. While others like age are intgers, and others are strings. \n",
    "\n",
    "Some features are translated into binary features. Gender is taken as a binary feature, with 'False' being taken as male and 'True' being taken female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "\n",
    "file = './Data/Autism/Autism-Adult-Data.arff'\n",
    "filedata, metadata = arff.loadarff(file)\n",
    "\n",
    "'''\n",
    "print (metadata)\n",
    "print (filedata[0])\n",
    "'''\n",
    "ethnicity = [b'?', b'White-European', b'Latino', b'Others', b'Black', b'Asian', b\"'Middle Eastern '\", b'Pasifika', b\"'South Asian'\", b'Hispanic', b'Turkish', b'others']\n",
    "country_of_res = [b\"'United States'\", b'Brazil', b'Spain', b'Egypt', b\"'New Zealand'\", b'Bahamas', b'Burundi', b'Austria', b'Argentina', b'Jordan', b'Ireland', b\"'United Arab Emirates'\", b'Afghanistan', b'Lebanon', b\"'United Kingdom'\", b\"'South Africa'\", b'Italy', b'Pakistan', b'Bangladesh', b'Chile', b'France', b'China', b'Australia', b'Canada', b\"'Saudi Arabia'\", b'Netherlands', b'Romania', b'Sweden', b'Tonga', b'Oman', b'India', b'Philippines', b\"'Sri Lanka'\", b\"'Sierra Leone'\", b'Ethiopia', b\"'Viet Nam'\", b'Iran', b\"'Costa Rica'\", b'Germany', b'Mexico', b'Russia', b'Armenia', b'Iceland', b'Nicaragua', b\"'Hong Kong'\", b'Japan', b'Ukraine', b'Kazakhstan', b'AmericanSamoa', b'Uruguay', b'Serbia', b'Portugal', b'Malaysia', b'Ecuador', b'Niger', b'Belgium', b'Bolivia', b'Aruba', b'Finland', b'Turkey', b'Nepal', b'Indonesia', b'Angola', b'Azerbaijan', b'Iraq', b\"'Czech Republic'\", b'Cyprus']\n",
    "relations = [b'?', b'Self', b'Parent', b\"'Health care professional'\", b'Relative', b'Others']\n",
    "\n",
    "# Data is not properly typed and needs to be converted\n",
    "data = [[None for _ in range(len(filedata[0]))] for _ in range(len(filedata))]\n",
    "for i in range(len(filedata)):\n",
    "    for j in range(len(filedata[i])):\n",
    "        # Binary features, Answer Scores\n",
    "        if (j < 10):\n",
    "            if (filedata[i][j] == b'1'):\n",
    "                data[i][j] = 1\n",
    "            else:\n",
    "                data[i][j] = 0\n",
    "        # Integer Features, Age feature, Screen Score\n",
    "        elif (j == 10 or j == 17):\n",
    "            data[i][j] = filedata[i][j]\n",
    "        # Gender Feature to binary\n",
    "        elif (j == 11):\n",
    "            if (filedata[i][j] == b'm'):\n",
    "                data[i][j] = 0\n",
    "            else:\n",
    "                data[i][j] = 1\n",
    "        # String features, Ethnicity, Country of Origin (enclossed in '' within the string), 18 or older, relation\n",
    "        elif (j == 12 or j == 15 or j == 18 or j == 19):\n",
    "            if (j == 12):\n",
    "                data[i][j] = ethnicity.index(filedata[i][j])\n",
    "            if (j == 15):\n",
    "                data[i][j] = country_of_res.index(filedata[i][j])\n",
    "            if (j == 19):\n",
    "                data[i][j] = relations.index(filedata[i][j])\n",
    "        # Jaundice, Family Member with a PDD, used the app before\n",
    "        elif (j == 13 or j == 14 or j == 16):\n",
    "            if (filedata[i][j] == b'yes'):\n",
    "                data[i][j] = 1\n",
    "            else:\n",
    "                data[i][j] = 0\n",
    "        # Final classification\n",
    "        elif (j == 20):\n",
    "            if (filedata[i][j] == b'YES'):\n",
    "                data[i][j] = 1\n",
    "            else:\n",
    "                data[i][j] = 0\n",
    "\n",
    "    # Make the row into a numpy array\n",
    "    data[i] = np.array(data[i])\n",
    "\n",
    "# Make the whole dataset into a numpy array\n",
    "data = np.array(data)\n",
    "\n",
    "# Transpose so that features are along the rows and data points are along the columns\n",
    "data = data.transpose()\n",
    "\n",
    "# Extract the label from the data\n",
    "labels = data[20:,:]\n",
    "data = data[:20,:]\n",
    "\n",
    "# data is now the features matrix column wise and the labels are separated into a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Dataset\n",
    "With the data properly loaded, we need to look over our features and potentially clean or adjust the data.\n",
    "\n",
    "In our dataset, there is an \"18 years or older\" feature, which is the same for every data point, a hold over from what can be assumed to be related to a legal obligation of an adult's consent to collect the data. Thus we can cut that feature out entirely from the beginning.\n",
    "\n",
    "Additionally, we can elect to exclude other features which we are not interested in including in our analysis such as .\n",
    "\n",
    "We can cut these out of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '18 years or older' feature\n",
    "data = np.delete(data, 18, axis=0)\n",
    "# Remove 'Used app before' feature\n",
    "data = np.delete(data, 16, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do I need to normalize the values?\n",
    "\n",
    "In some cases it may be necessary to normalize our features that we have selected. Lasso and Ridge regression put constraints on the size of the coefficients associated to each variable. However, this value will depend on the magnitude of each variable. It is therefore necessary to center and reduce, or standardize, the variables.\n",
    "\n",
    "However in the case of pure linear regression, where there are no constraints on the size of the coefficients, normalization is not necessary.\n",
    "\n",
    "In this example we have mostly binary features, with a couple of non-binary features like age, country of origin, ethnicity, and relation. For part 1, we will be doing a linear logistical regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 0 0 ... 1 0 1]\n",
      " ...\n",
      " [0 1 2 ... 40 17 66]\n",
      " [6.0 5.0 8.0 ... 7.0 6.0 8.0]\n",
      " [1 1 2 ... 0 1 1]]\n",
      "(18, 704)\n",
      "[0 1 1 1 1 1 0 0 1 0 17.0 1 0 0 0 5 6.0 0]\n",
      "[[1.0 1.0 1.0 ... 1.0 1.0 1.0]\n",
      " [1.0 1.0 1.0 ... 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 ... 1.0 0.0 1.0]\n",
      " ...\n",
      " [0.0 1.0 2.0 ... 40.0 17.0 66.0]\n",
      " [6.0 5.0 8.0 ... 7.0 6.0 8.0]\n",
      " [1.0 1.0 2.0 ... 0.0 1.0 1.0]]\n",
      "(18, 704)\n",
      "[0.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 0.0 17.0 1.0 0.0 0.0 0.0 5.0 6.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Convert integer features to floats\n",
    "print (data)\n",
    "print (data.shape)\n",
    "print (data.T[12])\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        data[i][j] = float(data[i][j])\n",
    "\n",
    "print (data)\n",
    "print (data.shape)\n",
    "print (data.T[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing our Cleaned Data\n",
    "\n",
    "This dataset required the most adjustment so that it can be used for the purposes of this project, mostly stemming from the use of an arff file for storing and distributing the data. In addition, not every feature was relevant as mentioned above. But now we can attempt to build a classifier around our data, to classify whether given ADS evaluation answers, and other factors, would lead to an ASD diagnosis. These are the 3 methods that will be used, along with the group member responsible for that method:\n",
    "\n",
    "- Logistical Regression - Brooks Tawil\n",
    "- Naive Bayes - Jack Chiu\n",
    "- Classification Method 3 - Gavin Mckim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression - Brooks Tawil\n",
    "\n",
    "In logistical regression, we want to restrict the output to a value between - and 1, and using some threshold value, classify someone as having an autism diagnosis as 'YES' or 'NO'. This binary decision comes from finding the weights for the typical linear regression  methods and then restricting this equation by a sigmoid:\n",
    "\n",
    "$\\dot{p} = \\frac{e^{b + w^Tx}}{1 + e^{b + w^Tx}}$\n",
    "\n",
    "Where $\\dot{p}$ is the probability that that $Y = 1$ for a given $X$.\n",
    "\n",
    "We start by doing linear regression as before and arrive at the weights using a k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a row of 1s for our linear regression\n",
    "X = np.append(data, np.ones((data.shape[1], 1)).transpose(), axis=0).T\n",
    "Y = np.array(labels).T\n",
    "\n",
    "# Will run a k-folds cross validation with k=10\n",
    "k = 10\n",
    "errors = []\n",
    "w_s = []\n",
    "count = 0\n",
    "\n",
    "# Allocate space for our folds and targets\n",
    "folds = [[] for _ in range(k)]\n",
    "targets = [[] for _ in range(k)]\n",
    "\n",
    "# Assign indices to the data for shuffling\n",
    "indices = []\n",
    "for i in range(len(X)):\n",
    "    indices.append(i)\n",
    "import random\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split the data and targets into seperate blocks\n",
    "count = 0\n",
    "for index in indices:    \n",
    "    folds[count].append(X[index])\n",
    "    targets[count].append(Y[index])\n",
    "    count += 1\n",
    "    if(count == k):\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now split into folds and can be used to fit linear models. We will first do so for the autism dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set 0 Error: 0.15684647862965836\n",
      "Test Set 1 Error: 0.17014421991693096\n",
      "Test Set 2 Error: 0.10629134615274191\n",
      "Test Set 3 Error: 2.1285357325967316\n",
      "Test Set 4 Error: 0.03612353157833446\n",
      "Test Set 5 Error: 0.01809964173089939\n",
      "Test Set 6 Error: 0.026117605126948943\n",
      "Test Set 7 Error: 0.03424264955584171\n",
      "Test Set 8 Error: 5.1095801834426355\n",
      "Test Set 9 Error: 0.19546626514148702\n",
      "Errors: [0.15684647862965836, 0.17014421991693096, 0.10629134615274191, 2.1285357325967316, 0.03612353157833446, 0.01809964173089939, 0.026117605126948943, 0.03424264955584171, 5.1095801834426355, 0.19546626514148702]\n",
      "Average regression error: \t0.798144765387221\n",
      "Weights: \n",
      " [[-2.88181774e-01]\n",
      " [-3.39464478e-01]\n",
      " [ 8.10485174e-01]\n",
      " [ 5.18968018e-02]\n",
      " [-1.63156598e-01]\n",
      " [ 4.41938329e-01]\n",
      " [ 1.32153666e-01]\n",
      " [ 2.37703684e-01]\n",
      " [ 3.11172364e-01]\n",
      " [ 8.94219616e-02]\n",
      " [ 9.72144235e-04]\n",
      " [ 3.47805630e-02]\n",
      " [-9.07144626e-03]\n",
      " [ 3.07350073e-02]\n",
      " [ 8.89698027e-03]\n",
      " [ 3.63926280e-04]\n",
      " [-8.11157227e-02]\n",
      " [-1.02574636e-02]\n",
      " [-3.88578136e-01]]\n",
      "Mean regression error using best w: 0.6241357238977943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Go through the folds of autsim test, form training and testing sets, fit a model, and calculate the error\n",
    "for i in range(len(folds)):\n",
    "    # Separate training_set\n",
    "    training_set = []\n",
    "    training_targets = []\n",
    "    for j in range(len(folds)):\n",
    "        if(i != j):\n",
    "            for l in range(len(folds[j])):\n",
    "                training_set.append(folds[j][l])\n",
    "                training_targets.append(targets[j][l])\n",
    "      \n",
    "    # From the test_set\n",
    "    test_set = np.array(folds[i]).astype(float)\n",
    "    test_targets = np.array(targets[i]).astype(float)\n",
    "    training_set = np.array(training_set).astype(float)\n",
    "    training_targets = np.array(training_targets).astype(float)\n",
    "    \n",
    "    # Run the LS regression on the training sets to get a w\n",
    "    w = np.linalg.inv(training_set.T @training_set) @ training_set.T @ training_targets\n",
    "    \n",
    "    # Use the found w to calculate y' on the test set\n",
    "    yHat = test_set @ w\n",
    "    \n",
    "    # Calculate the LS error \n",
    "    sumSquare = 0.0\n",
    "    for j in range(len(test_set)):\n",
    "        sumSquare += float(np.square(np.subtract(test_targets[i], yHat[i])))\n",
    "    sumSquare = sumSquare/len(test_set)\n",
    "        \n",
    "    print ('Test Set ' + str(i) + ' Error: ' + str(sumSquare))\n",
    "    errors.append(sumSquare)\n",
    "    \n",
    "    w_s.append(w)\n",
    "    \n",
    "print ('Errors: ' + str(errors))\n",
    "print ('Average regression error: \\t' + str(np.mean(errors)))\n",
    "\n",
    "# Get the w that gives the smallest error\n",
    "minError = np.inf\n",
    "minError_index = 0\n",
    "for i in range(len(errors)):\n",
    "    if errors[i] < minError:\n",
    "        minError = errors[i]\n",
    "        minError_index = i\n",
    "\n",
    "w_s = np.array(w_s).astype(float) \n",
    "best_w = w_s[minError_index]\n",
    "\n",
    "# With the best w, run across the whole set like before and find the error\n",
    "# Use the found w to calculate y' on the test set\n",
    "yHat = X.astype(float) @ best_w\n",
    "\n",
    "# Caluculate error using the best w\n",
    "sumSquare = 0\n",
    "for i in range(len(yHat)):\n",
    "    sumSquare += np.square(np.subtract(yHat[i], Y[i]))\n",
    "sumSquare = float(sumSquare/len(yHat))\n",
    "\n",
    "print ('Weights: \\n', best_w)\n",
    "print ('Mean regression error using best w: ' + str(sumSquare))\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the weight vector $w$ found, we can no model our regression model just as before. However now we restrict the output to be within $[0,1]$ using the sigmoid function described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2684659090909091, 0.2684659090909091, 0.2684659090909091, 0.2684659090909091, 0.2684659090909091, 0.2684659090909091, 0.2684659090909091, 0.2684659090909091, 0.2684659090909091, 0.2713068181818182, 0.27698863636363635, 0.28267045454545453, 0.2997159090909091, 0.33238636363636365, 0.37073863636363635, 0.3963068181818182, 0.44886363636363635, 0.5071022727272727, 0.5696022727272727, 0.6122159090909091, 0.6505681818181818, 0.6860795454545454, 0.7059659090909091, 0.7244318181818182, 0.7286931818181818, 0.7414772727272727, 0.7301136363636364, 0.71875, 0.7230113636363636, 0.7329545454545454, 0.734375, 0.7357954545454546, 0.7301136363636364, 0.7329545454545454, 0.7301136363636364, 0.7301136363636364, 0.7301136363636364, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909]\n",
      "0.5102040816326531\n"
     ]
    }
   ],
   "source": [
    "# For each of our datapoints, we can calculate a value of p\n",
    "# Where p is the probability that the data point results in a diagnosis of Autism.\n",
    "\n",
    "probability = np.exp(X.astype(float) @ best_w) / (1 + np.exp(X.astype(float) @ best_w))\n",
    "\n",
    "# Here we can set a threshold, alpha, and using cross validation arrive at an alpha that gives the error\n",
    "alphas = np.linspace(0.0, 1.0, num=50)\n",
    "classifications = [[None for _ in range(len(data[0]))] for _ in range(len(alphas))]\n",
    "\n",
    "# Assign the classes\n",
    "for i in range(len(alphas)):\n",
    "    for j in range(len(probability)):\n",
    "        # Check against threshold, give binary assignment of yes or no\n",
    "        if (probability[j] > alphas[i]):\n",
    "            classifications[i][j] = True\n",
    "        else:\n",
    "            classifications[i][j] = False\n",
    "            \n",
    "\n",
    "# True Positives, True Negatives, False Positives, and False Negatives for each of the alphas used\n",
    "TPs = [0 for _ in range(len(classifications))]\n",
    "TNs = [0 for _ in range(len(classifications))]\n",
    "FPs = [0 for _ in range(len(classifications))]\n",
    "FNs = [0 for _ in range(len(classifications))]\n",
    "\n",
    "# Take tallies of the TPs, TNs, FPs, FNs\n",
    "for i in range(len(classifications)):\n",
    "    for j in range(len(classifications[i])):\n",
    "        # True Positive\n",
    "        if (classifications[i][j] == Y[j] and Y[j] == True):\n",
    "            TPs[i] = TPs[i] + 1\n",
    "        # True Negative\n",
    "        elif (classifications[i][j] == Y[j] and Y[j] == False):\n",
    "            TNs[i] = TNs[i] + 1\n",
    "        # False Positive\n",
    "        elif (classifications[i][j] != Y[j] and Y[j] == False):\n",
    "            FPs[i] = FPs[i] + 1\n",
    "        # False Negative\n",
    "        else:\n",
    "            FNs[i] = FNs[i] + 1\n",
    "\n",
    "# For our purposes, lets select the alpha that gives the highest accuracy\n",
    "accuracies = [0 for _ in range(len(classifications))]\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    accuracies[i] = (TPs[i] + TNs[i])/float(len(probability))\n",
    "    \n",
    "print (accuracies)\n",
    "best_alpha = alphas[np.argmax(accuracies)]\n",
    "\n",
    "print (best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "Since we need to cross validate our alpha value for a binary classification, the use of an ROC curve would help in evaluating how good our classifier as a whole. Additionally, some values of alpha may give a higher classification error, but they may have a smaller false positive rate. As a machine learning engineer, its important to consider the effect that your algorithms have on people. Especially with diagnosis, we may aim to want to err on the side of caution, and have a higher false positive rate in order to minimize the false negative rate.\n",
    "\n",
    "Here we plot the ROC curve for a bunch of alpha thresholds, taking note that in some instances, it may be worth it to pick alpha values that favor more false positives. Whether a higher false positive rate is what is truly desired in these diagnosis is beyond the scope of this assignment as there are arguments to be made for both sides that we simply do not have enough knowledge, or time, to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.996116504854369, 0.9883495145631068, 0.9805825242718447, 0.9533980582524272, 0.9067961165048544, 0.8485436893203884, 0.8116504854368932, 0.7320388349514563, 0.6485436893203883, 0.5436893203883495, 0.46990291262135925, 0.4097087378640777, 0.34951456310679613, 0.287378640776699, 0.2446601941747573, 0.21165048543689322, 0.16116504854368932, 0.11844660194174757, 0.0970873786407767, 0.07961165048543689, 0.05242718446601942, 0.038834951456310676, 0.017475728155339806, 0.009708737864077669, 0.005825242718446602, 0.003883495145631068, 0.003883495145631068, 0.003883495145631068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9894179894179894, 0.9841269841269841, 0.9682539682539683, 0.9629629629629629, 0.9417989417989417, 0.9312169312169312, 0.8783068783068783, 0.8359788359788359, 0.8148148148148148, 0.783068783068783, 0.6878306878306878, 0.6402116402116402, 0.5661375661375662, 0.47619047619047616, 0.31746031746031744, 0.21693121693121692, 0.18518518518518517, 0.14814814814814814, 0.1164021164021164, 0.06349206349206349, 0.021164021164021163, 0.021164021164021163, 0.005291005291005291, 0.005291005291005291, 0.005291005291005291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-9012d46e4546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mroc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFPRs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTPRs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Logistical Regression ROC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFPRs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTPRs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'AUC = %0.2f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# First calculate the TPRs and FPRs\n",
    "FPRs = [0 for _ in range(len(classifications))]\n",
    "TPRs = [0 for _ in range(len(classifications))]\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    FPRs[i] = (FPs[i])/float(FPs[i] + TNs[i])\n",
    "    TPRs[i] = (TPs[i])/float(TPs[i] + FNs[i])\n",
    "    \n",
    "print (FPRs)\n",
    "print ()\n",
    "print (TPRs)\n",
    "\n",
    "# Uses the metrics package under sklearn to easily find the AUC\n",
    "from sklearn import metrics\n",
    "roc_auc = metrics.auc(FPRs, TPRs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.title('Logistical Regression ROC')\n",
    "plt.plot(FPRs, TPRs, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Jack Chui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bcbd484ac5e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0marff\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mARFF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0muNiQuE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpopCtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'arff'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import arff as ARFF\n",
    "\n",
    "def uNiQuE(vec):\n",
    "    popCtr = 0\n",
    "    for p in vec:\n",
    "        if p in vec[:popCtr]:\n",
    "            vec = np.delete(vec, popCtr, 0)\n",
    "            popCtr -= 1\n",
    "        popCtr += 1\n",
    "        \n",
    "    return vec\n",
    "\n",
    "def preProcess(data):\n",
    "    i = 0\n",
    "    for q in range(len(data)):\n",
    "        if None in data[i] or data[i,10] == 383:\n",
    "            data = np.delete(data, i, 0)\n",
    "            i -= 1        \n",
    "        i += 1\n",
    "\n",
    "        for j in range(len(data[i-1])):\n",
    "            if type(data[i-1,j]) == str:\n",
    "                data[i-1,j] = data[i-1,j].lower()\n",
    "    \n",
    "    labels = data[:,-1]\n",
    "    data = np.delete(data, -1, 1)\n",
    "    \n",
    "    ethnicities = uNiQuE(data[:,12])\n",
    "    countries = uNiQuE(data[:,15])\n",
    "    completed = uNiQuE(data[:,-1])\n",
    "    yesNo = np.array(['no', 'yes'])\n",
    "    ageVec = np.arange(20,80, 10)\n",
    "    for i in range(len(data)):\n",
    "        data[i,:10] = data[i,:10].astype(int)\n",
    "        \n",
    "        tempInd = np.where(ageVec > int(data[i,10]))\n",
    "        data[i,10] = int(tempInd[0][0])\n",
    "    \n",
    "        if data[i,11] == 'm':\n",
    "            data[i,11] = 1\n",
    "        else:\n",
    "            data[i,11] = 0\n",
    "        tempInd = np.where(ethnicities == data[i,12])\n",
    "        data[i,12] = int(tempInd[0])\n",
    "        \n",
    "        tempInd = np.where(yesNo == data[i,13])\n",
    "        data[i,13] = int(tempInd[0])\n",
    "        tempInd = np.where(yesNo == data[i,14])\n",
    "        data[i,14] = int(tempInd[0])\n",
    "        \n",
    "        tempInd = np.where(countries == data[i,15])\n",
    "        data[i,15] = int(tempInd[0])\n",
    "        \n",
    "        tempInd = np.where(yesNo == data[i,16])\n",
    "        data[i,16] = int(tempInd[0])\n",
    "        \n",
    "        data[i,17] = int(data[i,17])\n",
    "        \n",
    "        data[i,18] = 1\n",
    "        \n",
    "        tempInd =  np.where(completed==data[i,19])\n",
    "        data[i,19] = int(tempInd[0])\n",
    "        \n",
    "        labels[i] = int(labels[i] == 'yes')\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def kFold(data, labels, kFolds, typ = 'naive', k = 7):\n",
    "    #shuffle\n",
    "    inds = np.random.choice(np.arange(len(data)), len(data))\n",
    "    data[:] = data[inds]\n",
    "    labels[:] = labels[inds]\n",
    "\n",
    "    startInd = 0\n",
    "    stepSize = int(len(data)/kFolds)\n",
    "    max10 = max(data[:,10])\n",
    "    max12 = max(data[:,12])\n",
    "    max15 = max(data[:,15])\n",
    "    max17 = max(data[:,17])\n",
    "    max19 = max(data[:,19])\n",
    "    Errs = []\n",
    "    for i in range(kFolds):\n",
    "        if i != kFolds-1:\n",
    "            testData = data[startInd:startInd+stepSize]\n",
    "            testLabels = labels[startInd:startInd+stepSize]\n",
    "            trainingData = data[:startInd]\n",
    "            trainingData = np.concatenate((trainingData, data[startInd+stepSize:]))\n",
    "            trainingLabels = labels[:startInd]\n",
    "            trainingLabels = np.concatenate((trainingLabels,labels[startInd+stepSize:]))\n",
    "        else:\n",
    "            testData = data[startInd:]\n",
    "            testLabels = labels[startInd:]\n",
    "            trainingData = data[:startInd]\n",
    "            trainingLabels = labels[:startInd]\n",
    "        startInd += stepSize      \n",
    "        if typ == 'kNN':\n",
    "            trainingData[:,10] = trainingData[:,10]/max10\n",
    "            trainingData[:,12] = trainingData[:,12]/max12\n",
    "            trainingData[:,15] = trainingData[:,15]/max15\n",
    "            trainingData[:,17] = trainingData[:,17]/max17\n",
    "            trainingData[:,19] = trainingData[:,19]/max19\n",
    "            testData[:,10] = testData[:,10]/max10\n",
    "            testData[:,12] = testData[:,12]/max12\n",
    "            testData[:,15] = testData[:,15]/max15\n",
    "            testData[:,17] = testData[:,17]/max17\n",
    "            testData[:,19] = testData[:,19]/max19\n",
    "            temp = calcErr(trainingData, trainingLabels, testData, testLabels, k)\n",
    "        elif typ == 'naive':\n",
    "            temp = calcErrNaive(trainingData, trainingLabels, testData, testLabels)\n",
    "        \n",
    "        Errs.append(temp)\n",
    "    \n",
    "    return Errs\n",
    "        \n",
    "def trainNaive(data, labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    prior = np.array([counts[0], counts[1]])\n",
    "    prior = (prior+0.0)/len(data)\n",
    "    conditional = np.zeros((2, len(data[0]), 60))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            conditional[labels[i], j, data[i,j]] += 1\n",
    "            \n",
    "    for i in range(len(conditional)):\n",
    "        for j in range(len(conditional[0])):\n",
    "            sumCondition = sum(conditional[0,j]) + sum(conditional[1,j])\n",
    "            for k in range(len(conditional[0,j])):\n",
    "                conditional[i,j,k] = conditional[i,j,k]/sumCondition\n",
    "    \n",
    "    return prior, conditional, unique\n",
    "\n",
    "def testNaive(prior, conditional, unique, sample):\n",
    "    prob = np.array([prior[0],prior[1]])\n",
    "    for i in range(len(sample)):\n",
    "        prob[0] = prob[0] * conditional[0,i,sample[i]]\n",
    "        prob[1] = prob[1] * conditional[1,i,sample[i]]\n",
    "    \n",
    "    if prob[0] >= prob[1]:\n",
    "        return unique[0]\n",
    "        \n",
    "    return unique[1]\n",
    "        \n",
    "def calcErrNaive(trainingData, trainingLabels, testData, testLabels):\n",
    "    errs = 0\n",
    "    prior, conditional, unique = trainNaive(trainingData, trainingLabels)\n",
    "    for i in range(len(testData)):\n",
    "        prediction = testNaive(prior, conditional, unique, testData[i])\n",
    "        errs += int(prediction != testLabels[i])\n",
    "    \n",
    "    return np.round(1-errs/len(testLabels), 6)\n",
    "\n",
    "#file = 'Autism-Adult-Data.arff'\n",
    "dataset = ARFF.load(open(file))\n",
    "DATA = np.array(dataset['data'])    \n",
    "DATA, LABELS = preProcess(DATA)\n",
    "inds = np.random.choice(np.arange(len(DATA)), len(DATA))\n",
    "DATA[:] = DATA[inds]\n",
    "LABELS[:] = LABELS[inds]\n",
    "for i in np.arange(3,13,2):\n",
    "    kErrs = kFold(DATA, LABELS, i, typ = 'naive')\n",
    "    print('k = ', i, ' with average accuracy = ' , np.average(kErrs).round(6))\n",
    "    print('Accuracy for each fold: ', kErrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes was created to classify from the data whether the individual would potentially have Autism. Evidently, from running the classifier with k-fold cross validation over various k values, the accuracy of the classifier was high $\\geq 98\\%$. It can be said that there is a strong correlation between the collected data and whether an individual had autism or not based on the Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
