{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3 - Autism\n",
    "\n",
    "This dataset provides 20 attributes to help determine if an adult could be on the autistic spectrum or have ASD. The data provided is based upon autism screening of adults in contrast to most other datasets based on behavior traits. In the dataset, 10 behavioral and 10 individual characteristics are provided.\n",
    "https://archive.ics.uci.edu/ml/datasets/Autism+Screening+Adult\n",
    "\n",
    "#### Problem\n",
    "Our goal is to create a classifier, that can diagnose autism based on the answers to certain questions and physical characteristics. This is a classification problem and very much mirrors our hypothetical situations of diagnosing cancer that we discuss in class. Except our features are more based around psychological evaluation and not physical traits and attributes of a physical ailment.\n",
    "\n",
    "#### Extending the problem\n",
    "Out of all of the datasets, this one has the most social impact, and the creation of such an algorithm is most likely already the subject of study in various academic realms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "Autistic Spectrum Disorder (ASD) is a neurodevelopment  condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis.  The rapid growth in the number of ASD cases worldwide necessitates datasets related to behaviour traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilised for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioural features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behaviour science. \n",
    "\n",
    "Source: Fadi Fayez Thabtah\n",
    "Department of Digital Technology\n",
    "Manukau Institute of Technology,\n",
    "Auckland, New Zealand\n",
    "fadi.fayez@manukau.ac.nz\n",
    "\n",
    "Number of Instances (records in your data set): 704\n",
    "Number of Attributes (fields within each record): 21\n",
    "\n",
    "Attributes\n",
    "\n",
    "Age-Number-Age in years \n",
    "\n",
    "Gender-String-Male or Female \n",
    "\n",
    "Ethnicity-String-List of common ethnicities in text format\n",
    "\n",
    "Born with jaundice-Boolean  (yes or no)-Whether the case was born with jaundice\n",
    "\n",
    "Family member with PDD-Boolean  (yes or no)-Whether any immediate family member has a PDD\n",
    "\n",
    "Who is completing the test-String-Parent, self, caregiver, medical staff, clinician ,etc.\n",
    "Country of residence-String-List of countries in text format\n",
    "Used the screening app before-Boolean  (yes or no)-Whether the user has used a screening app\n",
    "Screening Method Type-Integer (0,1,2,3)\tThe type of screening methods chosen based on age category (0=toddler, 1=child, 2= adolescent, 3= adult)\n",
    "Question 1 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 2 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 3 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 4 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 5 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 6 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 7 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 8 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 9 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Question 10 Answer \tBinary (0, 1)\tThe answer code of the question based on the screening method used  \n",
    "Screening Score \tInteger \tThe final score obtained based on the scoring algorithm of the screening method used. This was computed in an automated manner\n",
    "\n",
    "\n",
    "Relevant Papers:  \n",
    "1) Tabtah, F. (2017). Autism Spectrum Disorder Screening: Machine Learning Adaptation and DSM-5 Fulfillment. Proceedings of the 1st International Conference on Medical and Health Informatics 2017, pp.1-6. Taichung City, Taiwan, ACM.\n",
    "2) Thabtah, F. (2017). ASDTests. A mobile app for ASD screening. www.asdtests.com [accessed December  20th, 2017].\n",
    "3) Thabtah, F. (2017). Machine Learning in Autistic Spectrum Disorder Behavioural Research: A Review. To Appear in Informatics for Health and Social Care Journal. December, 2017 (in press)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "The dataset is laid out in a .arff file and needs to be loaded properly so that we can use it. Fortunately, Scipy has a function to hanle this.\n",
    "\n",
    "Some features, like the answers to test questions, are binary. While others like age are intgers, and others are strings. \n",
    "\n",
    "Some features are translated into binary features. Gender is taken as a binary feature, with 'False' being taken as male and 'True' being taken female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "\n",
    "file = './Data/Autism/Autism-Adult-Data.arff'\n",
    "filedata, metadata = arff.loadarff(file)\n",
    "\n",
    "'''\n",
    "print (metadata)\n",
    "print (filedata[0])\n",
    "'''\n",
    "ethnicity = [b'?', b'White-European', b'Latino', b'Others', b'Black', b'Asian', b\"'Middle Eastern '\", b'Pasifika', b\"'South Asian'\", b'Hispanic', b'Turkish', b'others']\n",
    "country_of_res = [b\"'United States'\", b'Brazil', b'Spain', b'Egypt', b\"'New Zealand'\", b'Bahamas', b'Burundi', b'Austria', b'Argentina', b'Jordan', b'Ireland', b\"'United Arab Emirates'\", b'Afghanistan', b'Lebanon', b\"'United Kingdom'\", b\"'South Africa'\", b'Italy', b'Pakistan', b'Bangladesh', b'Chile', b'France', b'China', b'Australia', b'Canada', b\"'Saudi Arabia'\", b'Netherlands', b'Romania', b'Sweden', b'Tonga', b'Oman', b'India', b'Philippines', b\"'Sri Lanka'\", b\"'Sierra Leone'\", b'Ethiopia', b\"'Viet Nam'\", b'Iran', b\"'Costa Rica'\", b'Germany', b'Mexico', b'Russia', b'Armenia', b'Iceland', b'Nicaragua', b\"'Hong Kong'\", b'Japan', b'Ukraine', b'Kazakhstan', b'AmericanSamoa', b'Uruguay', b'Serbia', b'Portugal', b'Malaysia', b'Ecuador', b'Niger', b'Belgium', b'Bolivia', b'Aruba', b'Finland', b'Turkey', b'Nepal', b'Indonesia', b'Angola', b'Azerbaijan', b'Iraq', b\"'Czech Republic'\", b'Cyprus']\n",
    "relations = [b'?', b'Self', b'Parent', b\"'Health care professional'\", b'Relative', b'Others']\n",
    "\n",
    "# Data is not properly typed and needs to be converted\n",
    "data = [[None for _ in range(len(filedata[0]))] for _ in range(len(filedata))]\n",
    "for i in range(len(filedata)):\n",
    "    for j in range(len(filedata[i])):\n",
    "        # Binary features, Answer Scores\n",
    "        if (j < 10):\n",
    "            if (filedata[i][j] == b'1'):\n",
    "                data[i][j] = 1\n",
    "            else:\n",
    "                data[i][j] = 0\n",
    "        # Integer Features, Age feature, Screen Score\n",
    "        elif (j == 10 or j == 17):\n",
    "            data[i][j] = filedata[i][j]\n",
    "        # Gender Feature to binary\n",
    "        elif (j == 11):\n",
    "            if (filedata[i][j] == b'm'):\n",
    "                data[i][j] = 0\n",
    "            else:\n",
    "                data[i][j] = 1\n",
    "        # String features, Ethnicity, Country of Origin (enclossed in '' within the string), 18 or older, relation\n",
    "        elif (j == 12 or j == 15 or j == 18 or j == 19):\n",
    "            if (j == 12):\n",
    "                data[i][j] = ethnicity.index(filedata[i][j])\n",
    "            if (j == 15):\n",
    "                data[i][j] = country_of_res.index(filedata[i][j])\n",
    "            if (j == 19):\n",
    "                data[i][j] = relations.index(filedata[i][j])\n",
    "        # Jaundice, Family Member with a PDD, used the app before\n",
    "        elif (j == 13 or j == 14 or j == 16):\n",
    "            if (filedata[i][j] == b'yes'):\n",
    "                data[i][j] = 1\n",
    "            else:\n",
    "                data[i][j] = 0\n",
    "        # Final classification\n",
    "        elif (j == 20):\n",
    "            if (filedata[i][j] == b'YES'):\n",
    "                data[i][j] = 1\n",
    "            else:\n",
    "                data[i][j] = 0\n",
    "\n",
    "    # Make the row into a numpy array\n",
    "    data[i] = np.array(data[i])\n",
    "\n",
    "# Make the whole dataset into a numpy array\n",
    "data = np.array(data)\n",
    "\n",
    "# Transpose so that features are along the rows and data points are along the columns\n",
    "data = data.transpose()\n",
    "\n",
    "# Extract the label from the data\n",
    "labels = data[20:,:]\n",
    "data = data[:20,:]\n",
    "\n",
    "# data is now the features matrix column wise and the labels are separated into a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Dataset\n",
    "With the data properly loaded, we need to look over our features and potentially clean or adjust the data.\n",
    "\n",
    "In our dataset, there is an \"18 years or older\" feature, which is the same for every data point, a hold over from what can be assumed to be related to a legal obligation of an adult's consent to collect the data. Thus we can cut that feature out entirely from the beginning.\n",
    "\n",
    "Additionally, we can elect to exclude other features which we are not interested in including in our analysis such as .\n",
    "\n",
    "We can cut these out of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '18 years or older' feature\n",
    "data = np.delete(data, 18, axis=0)\n",
    "# Remove 'Used app before' feature\n",
    "data = np.delete(data, 16, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do I need to normalize the values?\n",
    "\n",
    "In some cases it may be necessary to normalize our features that we have selected. Lasso and Ridge regression put constraints on the size of the coefficients associated to each variable. However, this value will depend on the magnitude of each variable. It is therefore necessary to center and reduce, or standardize, the variables.\n",
    "\n",
    "However in the case of pure linear regression, where there are no constraints on the size of the coefficients, normalization is not necessary.\n",
    "\n",
    "In this example we have mostly binary features, with a couple of non-binary features like age, country of origin, ethnicity, and relation. For part 1, we will be doing a linear logistical regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [1 0 0 ... 1 0 1]\n",
      " ...\n",
      " [0 1 2 ... 40 17 66]\n",
      " [6.0 5.0 8.0 ... 7.0 6.0 8.0]\n",
      " [1 1 2 ... 0 1 1]]\n",
      "(18, 704)\n",
      "[0 1 1 1 1 1 0 0 1 0 17.0 1 0 0 0 5 6.0 0]\n",
      "[[1.0 1.0 1.0 ... 1.0 1.0 1.0]\n",
      " [1.0 1.0 1.0 ... 0.0 0.0 0.0]\n",
      " [1.0 0.0 0.0 ... 1.0 0.0 1.0]\n",
      " ...\n",
      " [0.0 1.0 2.0 ... 40.0 17.0 66.0]\n",
      " [6.0 5.0 8.0 ... 7.0 6.0 8.0]\n",
      " [1.0 1.0 2.0 ... 0.0 1.0 1.0]]\n",
      "(18, 704)\n",
      "[0.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 1.0 0.0 17.0 1.0 0.0 0.0 0.0 5.0 6.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Convert integer features to floats\n",
    "print (data)\n",
    "print (data.shape)\n",
    "print (data.T[12])\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        data[i][j] = float(data[i][j])\n",
    "\n",
    "print (data)\n",
    "print (data.shape)\n",
    "print (data.T[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing our Cleaned Data\n",
    "\n",
    "This dataset required the most adjustment so that it can be used for the purposes of this project, mostly stemming from the use of an arff file for storing and distributing the data. In addition, not every feature was relevant as mentioned above. But now we can attempt to build a classifier around our data, to classify whether given ADS evaluation answers, and other factors, would lead to an ASD diagnosis. These are the 3 methods that will be used, along with the group member responsible for that method:\n",
    "\n",
    "- Logistical Regression - Brooks Tawil\n",
    "- Naive Bayes - Jack Chiu\n",
    "- Linear SVM - Gavin Mckim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistical Regression - Brooks Tawil\n",
    "\n",
    "In logistical regression, we want to restrict the output to a value between - and 1, and using some threshold value, classify someone as having an autism diagnosis as 'YES' or 'NO'. This binary decision comes from finding the weights for the typical linear regression  methods and then restricting this equation by a sigmoid:\n",
    "\n",
    "$\\dot{p} = \\frac{e^{b + w^Tx}}{1 + e^{b + w^Tx}}$\n",
    "\n",
    "Where $\\dot{p}$ is the probability that that $Y = 1$ for a given $X$.\n",
    "\n",
    "We start by doing linear regression as before and arrive at the weights using a k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a row of 1s for our linear regression\n",
    "X = np.append(data, np.ones((data.shape[1], 1)).transpose(), axis=0).T\n",
    "Y = np.array(labels).T\n",
    "\n",
    "# Will run a k-folds cross validation with k=15\n",
    "k = 15\n",
    "errors = []\n",
    "w_s = []\n",
    "count = 0\n",
    "\n",
    "# Allocate space for our folds and targets\n",
    "folds = [[] for _ in range(k)]\n",
    "targets = [[] for _ in range(k)]\n",
    "\n",
    "# Assign indices to the data for shuffling\n",
    "indices = []\n",
    "for i in range(len(X)):\n",
    "    indices.append(i)\n",
    "import random\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split the data and targets into seperate blocks\n",
    "count = 0\n",
    "for index in indices:    \n",
    "    folds[count].append(X[index])\n",
    "    targets[count].append(Y[index])\n",
    "    count += 1\n",
    "    if(count == k):\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now split into folds and can be used to fit linear models. We will first do so for the autism dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set 0 Error: 15.892721661243087\n",
      "Test Set 1 Error: 0.7579867654864171\n",
      "Test Set 2 Error: 11.354681261175637\n",
      "Test Set 3 Error: 2.4858906278584847\n",
      "Test Set 4 Error: 2.540176242755948\n",
      "Test Set 5 Error: 12.337593465146865\n",
      "Test Set 6 Error: 0.3758079828813586\n",
      "Test Set 7 Error: 0.4042145538837417\n",
      "Test Set 8 Error: 0.08964161559242946\n",
      "Test Set 9 Error: 58.557953873309096\n",
      "Test Set 10 Error: 2.133886167082058\n",
      "Test Set 11 Error: 0.003346948871208241\n",
      "Test Set 12 Error: 5.27504805987291\n",
      "Test Set 13 Error: 0.41088865923918366\n",
      "Test Set 14 Error: 227.4604040847573\n",
      "Errors: [15.892721661243087, 0.7579867654864171, 11.354681261175637, 2.4858906278584847, 2.540176242755948, 12.337593465146865, 0.3758079828813586, 0.4042145538837417, 0.08964161559242946, 58.557953873309096, 2.133886167082058, 0.003346948871208241, 5.27504805987291, 0.41088865923918366, 227.4604040847573]\n",
      "Average regression error: \t22.67201613127705\n",
      "Weights: \n",
      " [[ 1.33265843e+00]\n",
      " [ 3.13949938e-01]\n",
      " [-8.89758834e-01]\n",
      " [ 3.82608685e-03]\n",
      " [-1.34932501e+00]\n",
      " [ 1.00312211e+00]\n",
      " [ 4.14714104e-01]\n",
      " [ 5.75799926e-01]\n",
      " [ 5.48127182e-02]\n",
      " [-2.68360010e-02]\n",
      " [ 9.53001218e-04]\n",
      " [ 3.03155171e-02]\n",
      " [-5.86877860e-03]\n",
      " [ 3.88135727e-02]\n",
      " [ 1.31741459e-02]\n",
      " [-1.21599504e-04]\n",
      " [-2.28613270e-01]\n",
      " [-1.38704132e-02]\n",
      " [-3.71013431e-01]]\n",
      "Mean regression error using best w: 2.577708035664114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Go through the folds of autsim test, form training and testing sets, fit a model, and calculate the error\n",
    "for i in range(len(folds)):\n",
    "    # Separate training_set\n",
    "    training_set = []\n",
    "    training_targets = []\n",
    "    for j in range(len(folds)):\n",
    "        if(i != j):\n",
    "            for l in range(len(folds[j])):\n",
    "                training_set.append(folds[j][l])\n",
    "                training_targets.append(targets[j][l])\n",
    "      \n",
    "    # From the test_set\n",
    "    test_set = np.array(folds[i]).astype(float)\n",
    "    test_targets = np.array(targets[i]).astype(float)\n",
    "    training_set = np.array(training_set).astype(float)\n",
    "    training_targets = np.array(training_targets).astype(float)\n",
    "    \n",
    "    # Run the LS regression on the training sets to get a w\n",
    "    w = np.linalg.inv(training_set.T @training_set) @ training_set.T @ training_targets\n",
    "    \n",
    "    # Use the found w to calculate y' on the test set\n",
    "    yHat = test_set @ w\n",
    "    \n",
    "    # Calculate the LS error \n",
    "    sumSquare = 0.0\n",
    "    for j in range(len(test_set)):\n",
    "        sumSquare += float(np.square(np.subtract(test_targets[i], yHat[i])))\n",
    "    sumSquare = sumSquare/len(test_set)\n",
    "        \n",
    "    print ('Test Set ' + str(i) + ' Error: ' + str(sumSquare))\n",
    "    errors.append(sumSquare)\n",
    "    \n",
    "    w_s.append(w)\n",
    "    \n",
    "print ('Errors: ' + str(errors))\n",
    "print ('Average regression error: \\t' + str(np.mean(errors)))\n",
    "\n",
    "# Get the w that gives the smallest error\n",
    "best_w = w_s[np.argmin(errors)]\n",
    "\n",
    "# With the best w, run across the whole set like before and find the error\n",
    "# Use the found w to calculate y' on the test set\n",
    "yHat = X.astype(float) @ best_w\n",
    "\n",
    "# Caluculate error using the best w\n",
    "sumSquare = 0\n",
    "for i in range(len(yHat)):\n",
    "    sumSquare += np.square(np.subtract(yHat[i], Y[i]))\n",
    "sumSquare = float(sumSquare/len(yHat))\n",
    "\n",
    "print ('Weights: \\n', best_w)\n",
    "print ('Mean regression error using best w: ' + str(sumSquare))\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the weight vector $w$ found, we can no model our regression model just as before. However now we restrict the output to be within $[0,1]$ using the sigmoid function described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2684659090909091, 0.2684659090909091, 0.2869318181818182, 0.29261363636363635, 0.29829545454545453, 0.30113636363636365, 0.2897727272727273, 0.2940340909090909, 0.3096590909090909, 0.3181818181818182, 0.30823863636363635, 0.3025568181818182, 0.26420454545454547, 0.2684659090909091, 0.28267045454545453, 0.2883522727272727, 0.3053977272727273, 0.31676136363636365, 0.3465909090909091, 0.3693181818181818, 0.3934659090909091, 0.4289772727272727, 0.44176136363636365, 0.4559659090909091, 0.47017045454545453, 0.5028409090909091, 0.5127840909090909, 0.5113636363636364, 0.515625, 0.5142045454545454, 0.5213068181818182, 0.5284090909090909, 0.5383522727272727, 0.5497159090909091, 0.5880681818181818, 0.6178977272727273, 0.6335227272727273, 0.6761363636363636, 0.7017045454545454, 0.7173295454545454, 0.7215909090909091, 0.7244318181818182, 0.7272727272727273, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909, 0.7315340909090909]\n",
      "\n",
      "Best Alpha:  0.8775510204081632\n"
     ]
    }
   ],
   "source": [
    "# For each of our datapoints, we can calculate a value of p\n",
    "# Where p is the probability that the data point results in a diagnosis of Autism.\n",
    "\n",
    "probability = np.exp(X.astype(float) @ best_w) / (1 + np.exp(X.astype(float) @ best_w))\n",
    "\n",
    "# Here we can set a threshold, alpha, and using cross validation arrive at an alpha that gives the error\n",
    "alphas = np.linspace(0.0, 1.0, num=50)\n",
    "classifications = [[None for _ in range(len(data[0]))] for _ in range(len(alphas))]\n",
    "\n",
    "# Assign the classes\n",
    "for i in range(len(alphas)):\n",
    "    for j in range(len(probability)):\n",
    "        # Check against threshold, give binary assignment of yes or no\n",
    "        if (probability[j] > alphas[i]):\n",
    "            classifications[i][j] = True\n",
    "        else:\n",
    "            classifications[i][j] = False\n",
    "            \n",
    "\n",
    "# True Positives, True Negatives, False Positives, and False Negatives for each of the alphas used\n",
    "TPs = [0 for _ in range(len(classifications))]\n",
    "TNs = [0 for _ in range(len(classifications))]\n",
    "FPs = [0 for _ in range(len(classifications))]\n",
    "FNs = [0 for _ in range(len(classifications))]\n",
    "\n",
    "# Take tallies of the TPs, TNs, FPs, FNs\n",
    "for i in range(len(classifications)):\n",
    "    for j in range(len(classifications[i])):\n",
    "        # True Positive\n",
    "        if (classifications[i][j] == Y[j] and Y[j] == True):\n",
    "            TPs[i] = TPs[i] + 1\n",
    "        # True Negative\n",
    "        elif (classifications[i][j] == Y[j] and Y[j] == False):\n",
    "            TNs[i] = TNs[i] + 1\n",
    "        # False Positive\n",
    "        elif (classifications[i][j] != Y[j] and Y[j] == False):\n",
    "            FPs[i] = FPs[i] + 1\n",
    "        # False Negative\n",
    "        else:\n",
    "            FNs[i] = FNs[i] + 1\n",
    "\n",
    "# For our purposes, lets select the alpha that gives the highest accuracy\n",
    "accuracies = [0 for _ in range(len(classifications))]\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    accuracies[i] = (TPs[i] + TNs[i])/float(len(probability))\n",
    "    \n",
    "print (accuracies)\n",
    "best_alpha = alphas[np.argmax(accuracies)]\n",
    "\n",
    "print()\n",
    "print ('Best Alpha: ', best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "Since we need to cross validate our alpha value for a binary classification, the use of an ROC curve would help in evaluating how good our classifier as a whole. Additionally, some values of alpha may give a higher classification error, but they may have a smaller false positive rate. As a machine learning engineer, its important to consider the effect that your algorithms have on people. Especially with diagnosis, we may aim to want to err on the side of caution, and have a higher false positive rate in order to minimize the false negative rate.\n",
    "\n",
    "Here we plot the ROC curve for a bunch of alpha thresholds, taking note that in some instances, it may be worth it to pick alpha values that favor more false positives. Whether a higher false positive rate is what is truly desired in these diagnosis is beyond the scope of this assignment as there are arguments to be made for both sides that we simply do not have enough knowledge, or time, to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First calculate the TPRs and FPRs\n",
    "FPRs = [0 for _ in range(len(classifications))]\n",
    "TPRs = [0 for _ in range(len(classifications))]\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    FPRs[i] = (FPs[i])/float(FPs[i] + TNs[i])\n",
    "    TPRs[i] = (TPs[i])/float(TPs[i] + FNs[i])\n",
    "\n",
    "# Uses the metrics package under sklearn to easily find the AUC\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "roc_auc = metrics.auc(FPRs, TPRs)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Logistical Regression ROC')\n",
    "plt.plot(FPRs, TPRs, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Jack Chiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  3  with average accuracy =  0.983595\n",
      "Accuracy for each fold:  [0.990099, 0.990099, 0.970588]\n",
      "k =  5  with average accuracy =  0.985244\n",
      "Accuracy for each fold:  [1.0, 0.975207, 0.983471, 0.991736, 0.975806]\n",
      "k =  7  with average accuracy =  0.990141\n",
      "Accuracy for each fold:  [1.0, 0.976744, 1.0, 0.976744, 0.988372, 1.0, 0.98913]\n",
      "k =  9  with average accuracy =  0.991823\n",
      "Accuracy for each fold:  [1.0, 1.0, 0.985075, 0.985075, 1.0, 1.0, 1.0, 0.970149, 0.986111]\n",
      "k =  11  with average accuracy =  0.990168\n",
      "Accuracy for each fold:  [0.981818, 0.981818, 1.0, 1.0, 0.963636, 1.0, 1.0, 0.981818, 1.0, 1.0, 0.982759]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import arff as ARFF\n",
    "\n",
    "def uNiQuE(vec):\n",
    "    popCtr = 0\n",
    "    for p in vec:\n",
    "        if p in vec[:popCtr]:\n",
    "            vec = np.delete(vec, popCtr, 0)\n",
    "            popCtr -= 1\n",
    "        popCtr += 1\n",
    "        \n",
    "    return vec\n",
    "\n",
    "def preProcess(data):\n",
    "    i = 0\n",
    "    for q in range(len(data)):\n",
    "        if None in data[i] or data[i,10] == 383:\n",
    "            data = np.delete(data, i, 0)\n",
    "            i -= 1        \n",
    "        i += 1\n",
    "\n",
    "        for j in range(len(data[i-1])):\n",
    "            if type(data[i-1,j]) == str:\n",
    "                data[i-1,j] = data[i-1,j].lower()\n",
    "    \n",
    "    labels = data[:,-1]\n",
    "    data = np.delete(data, -1, 1)\n",
    "    \n",
    "    ethnicities = uNiQuE(data[:,12])\n",
    "    countries = uNiQuE(data[:,15])\n",
    "    completed = uNiQuE(data[:,-1])\n",
    "    yesNo = np.array(['no', 'yes'])\n",
    "    ageVec = np.arange(20,80, 10)\n",
    "    for i in range(len(data)):\n",
    "        data[i,:10] = data[i,:10].astype(int)\n",
    "        \n",
    "        tempInd = np.where(ageVec > int(data[i,10]))\n",
    "        data[i,10] = int(tempInd[0][0])\n",
    "    \n",
    "        if data[i,11] == 'm':\n",
    "            data[i,11] = 1\n",
    "        else:\n",
    "            data[i,11] = 0\n",
    "        tempInd = np.where(ethnicities == data[i,12])\n",
    "        data[i,12] = int(tempInd[0])\n",
    "        \n",
    "        tempInd = np.where(yesNo == data[i,13])\n",
    "        data[i,13] = int(tempInd[0])\n",
    "        tempInd = np.where(yesNo == data[i,14])\n",
    "        data[i,14] = int(tempInd[0])\n",
    "        \n",
    "        tempInd = np.where(countries == data[i,15])\n",
    "        data[i,15] = int(tempInd[0])\n",
    "        \n",
    "        tempInd = np.where(yesNo == data[i,16])\n",
    "        data[i,16] = int(tempInd[0])\n",
    "        \n",
    "        data[i,17] = int(data[i,17])\n",
    "        \n",
    "        data[i,18] = 1\n",
    "        \n",
    "        tempInd =  np.where(completed==data[i,19])\n",
    "        data[i,19] = int(tempInd[0])\n",
    "        \n",
    "        labels[i] = int(labels[i] == 'yes')\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def kFold(data, labels, kFolds, typ = 'naive', k = 7):\n",
    "    #shuffle\n",
    "    inds = np.random.choice(np.arange(len(data)), len(data))\n",
    "    data[:] = data[inds]\n",
    "    labels[:] = labels[inds]\n",
    "\n",
    "    startInd = 0\n",
    "    stepSize = int(len(data)/kFolds)\n",
    "    max10 = max(data[:,10])\n",
    "    max12 = max(data[:,12])\n",
    "    max15 = max(data[:,15])\n",
    "    max17 = max(data[:,17])\n",
    "    max19 = max(data[:,19])\n",
    "    Errs = []\n",
    "    for i in range(kFolds):\n",
    "        if i != kFolds-1:\n",
    "            testData = data[startInd:startInd+stepSize]\n",
    "            testLabels = labels[startInd:startInd+stepSize]\n",
    "            trainingData = data[:startInd]\n",
    "            trainingData = np.concatenate((trainingData, data[startInd+stepSize:]))\n",
    "            trainingLabels = labels[:startInd]\n",
    "            trainingLabels = np.concatenate((trainingLabels,labels[startInd+stepSize:]))\n",
    "        else:\n",
    "            testData = data[startInd:]\n",
    "            testLabels = labels[startInd:]\n",
    "            trainingData = data[:startInd]\n",
    "            trainingLabels = labels[:startInd]\n",
    "        startInd += stepSize      \n",
    "        if typ == 'kNN':\n",
    "            trainingData[:,10] = trainingData[:,10]/max10\n",
    "            trainingData[:,12] = trainingData[:,12]/max12\n",
    "            trainingData[:,15] = trainingData[:,15]/max15\n",
    "            trainingData[:,17] = trainingData[:,17]/max17\n",
    "            trainingData[:,19] = trainingData[:,19]/max19\n",
    "            testData[:,10] = testData[:,10]/max10\n",
    "            testData[:,12] = testData[:,12]/max12\n",
    "            testData[:,15] = testData[:,15]/max15\n",
    "            testData[:,17] = testData[:,17]/max17\n",
    "            testData[:,19] = testData[:,19]/max19\n",
    "            temp = calcErr(trainingData, trainingLabels, testData, testLabels, k)\n",
    "        elif typ == 'naive':\n",
    "            temp = calcErrNaive(trainingData, trainingLabels, testData, testLabels)\n",
    "        \n",
    "        Errs.append(temp)\n",
    "    \n",
    "    return Errs\n",
    "        \n",
    "def trainNaive(data, labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    prior = np.array([counts[0], counts[1]])\n",
    "    prior = (prior+0.0)/len(data)\n",
    "    conditional = np.zeros((2, len(data[0]), 60))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            conditional[labels[i], j, data[i,j]] += 1\n",
    "            \n",
    "    for i in range(len(conditional)):\n",
    "        for j in range(len(conditional[0])):\n",
    "            sumCondition = sum(conditional[0,j]) + sum(conditional[1,j])\n",
    "            for k in range(len(conditional[0,j])):\n",
    "                conditional[i,j,k] = conditional[i,j,k]/sumCondition\n",
    "    \n",
    "    return prior, conditional, unique\n",
    "\n",
    "def testNaive(prior, conditional, unique, sample):\n",
    "    prob = np.array([prior[0],prior[1]])\n",
    "    for i in range(len(sample)):\n",
    "        prob[0] = prob[0] * conditional[0,i,sample[i]]\n",
    "        prob[1] = prob[1] * conditional[1,i,sample[i]]\n",
    "    \n",
    "    if prob[0] >= prob[1]:\n",
    "        return unique[0]\n",
    "        \n",
    "    return unique[1]\n",
    "        \n",
    "def calcErrNaive(trainingData, trainingLabels, testData, testLabels):\n",
    "    errs = 0\n",
    "    prior, conditional, unique = trainNaive(trainingData, trainingLabels)\n",
    "    for i in range(len(testData)):\n",
    "        prediction = testNaive(prior, conditional, unique, testData[i])\n",
    "        errs += int(prediction != testLabels[i])\n",
    "    \n",
    "    return np.round(1-errs/len(testLabels), 6)\n",
    "\n",
    "#file = 'Autism-Adult-Data.arff'\n",
    "dataset = ARFF.load(open(file))\n",
    "DATA = np.array(dataset['data'])    \n",
    "DATA, LABELS = preProcess(DATA)\n",
    "inds = np.random.choice(np.arange(len(DATA)), len(DATA))\n",
    "DATA[:] = DATA[inds]\n",
    "LABELS[:] = LABELS[inds]\n",
    "for i in np.arange(3,13,2):\n",
    "    kErrs = kFold(DATA, LABELS, i, typ = 'naive')\n",
    "    print('k = ', i, ' with average accuracy = ' , np.average(kErrs).round(6))\n",
    "    print('Accuracy for each fold: ', kErrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes was created to classify from the data whether the individual would potentially have Autism. Evidently, from running the classifier with k-fold cross validation over various k values, the accuracy of the classifier was high $\\geq 98\\%$. It can be said that there is a strong correlation between the collected data and whether an individual had autism or not based on the Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM - Gavin McKim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "sdata = data\n",
    "shfl = np.vstack((sdata,np.reshape(labels,[1,704])))\n",
    "shfl = np.random.permutation(shfl.T).T\n",
    "sdata = shfl[0:18,:]\n",
    "labels = shfl[18,:]\n",
    "\n",
    "label=label.astype('int')\n",
    "means = []\n",
    "C = np.arange(.1,2,0.1)\n",
    "for c in C:\n",
    "    \n",
    "    clf = svm.SVC(C=c, kernel = 'linear', gamma = 'auto')\n",
    "\n",
    "    acc = []\n",
    "\n",
    "\n",
    "    # 5-Fold Cross Validation\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            testing = sdata[:,0:141]\n",
    "            training = sdata[:,141:704]\n",
    "            yf = label[141:704]\n",
    "            yt = label[0:141]\n",
    "        if i == 1:\n",
    "            testing = sdata[:,141:282]\n",
    "            training = np.concatenate((sdata[:,0:141],sdata[:,282:704]), axis=1)\n",
    "            yf = np.concatenate((label[0:141],label[282:704]), axis=0)\n",
    "            yt = label[141:282]\n",
    "        if i == 2:\n",
    "            testing = sdata[:,282:423]\n",
    "            training = np.concatenate((sdata[:,0:282],sdata[:,423:704]), axis=1)\n",
    "            yf = np.concatenate((label[0:282],label[423:704]), axis=0)\n",
    "            yt = label[282:423]\n",
    "        if i == 3:\n",
    "            testing = sdata[:,423:564]\n",
    "            training = np.concatenate((sdata[:,0:423],sdata[:,564:704]), axis=1)\n",
    "            yf = np.concatenate((label[0:423],label[564:704]), axis=0)\n",
    "            yt = label[423:564]\n",
    "        if i == 4:\n",
    "            testing = sdata[:,564:704]\n",
    "            training = sdata[:,0:564]\n",
    "            yf = label[0:564]\n",
    "            yt = label[564:704]\n",
    "\n",
    "        clf.fit(training.T, yf)\n",
    "        acc.append(clf.score(testing.T,yt))\n",
    "    means.append(np.mean(acc))\n",
    "\n",
    "opt = np.argmax(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy is: 0.7314690982776091\n",
      "This accuracy is achieved by having a penalty term of: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"The highest accuracy is:\",means[opt])\n",
    "print(\"This accuracy is achieved by having a penalty term of:\", (opt/10)+.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",

   "version": "3.7.1"

  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
